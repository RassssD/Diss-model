{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random, math\n",
    "import graphviz\n",
    "import functools, collections, operator\n",
    "\n",
    "import time\n",
    "\n",
    "import kaleido\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# import models\n",
    "#from classes.jackson_model import JacksonSimulationV2\n",
    "from classes.network_drawing import plotly_sim_drawing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jackson model\n",
    "* Start with m nodes\n",
    "* New nodes are introduced to m nodes\n",
    "* Befriend with probability pm\n",
    "* New nodes are then introduced to n of those pm nodes' connections\n",
    "* Befriend with probability pn\n",
    "* Continue for as many iterations as would like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation version 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates:\n",
    "* Attributes of encountered nodes (i.e., the exposure group)\n",
    "* CORRECTED: will now find parent neighbours even if not connected to parents\n",
    "* Rewrtitten connection search functions to include two stages for easier access to targets/connection group\n",
    "* Add connection chance as f of cross/within\n",
    "* Create function to check number of cross/within depending on the group\n",
    "* Caluclate SES assortativity\n",
    "\n",
    "TODO\n",
    "\n",
    "High SES individuals have 25.4% more friends on average\n",
    "\n",
    "Create functions for\n",
    "* Calculate average bias and exposure for nodes in the network\n",
    "* Calculate all 5 characteristics of real social networks\n",
    "* Find a measure of segregation on networks?\n",
    "* Making samples / lots of simulations, and timing them \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JacksonSimulationV2():\n",
    "\n",
    "    def __init__(self, initial_n, T, m, pm_o, n, pn_o, p_SES_high, biased = False, rho = 1):\n",
    "\n",
    "        # parameters\n",
    "        self.initial_n = initial_n # initial number of nodes\n",
    "        self.T = T # number of iterations / final nodes - initial_n\n",
    "        self.m = m # number of parent nodes encountered\n",
    "        self.pm_o = pm_o # probability of befriending a parent node of same SES\n",
    "        self.n = n # number of parent neighbour nodes encountered\n",
    "        self.pn_o = pn_o # probability of befriending parent neighbour nodes of same SES\n",
    "        self.p_SES_high = p_SES_high # probability of each node's SES to be High\n",
    "        self.biased = biased # whether or not to use a class-biased simulation\n",
    "        self.rho = rho # ratio (within friendship probatility)/(cross friendship probatility): rho = how many times more likely to befriend within-class than across-class (rho = 2 => twice as likely, rho = 1/3 => three times less likely)\n",
    "        # NB rho is initialised as 1, hence no bias, per default. If biased, specify rho value\n",
    "        \n",
    "        # calculate cross-friendship probabilities: rho = po/px => px = po/rho\n",
    "        self.pm_x = pm_o / rho\n",
    "        self.pn_x = pn_o / rho\n",
    "\n",
    "        if biased == True and rho == 1:\n",
    "            print('Warning: Biased simulation requested, but rho = 1.')\n",
    "        if biased == False and rho != 1:\n",
    "            print(f'Warning: Unbiased simulation requested, but rho = {self.rho}')\n",
    "\n",
    "        # also maybe add options for the functions used\n",
    "        # self.initialiser_function = \n",
    "\n",
    "        # simulation memory\n",
    "        self.period = 0\n",
    "        self.graph_history = []\n",
    "        # will also need a matrix history to make it easier to save simulations to a csv or the like\n",
    "\n",
    "        # starting stuff\n",
    "        #self.initial_nodes = [i+1 for i in range(initial_n)] # create n initial nodes, +1 for name to start at 1\n",
    "\n",
    "        self.main()\n",
    "\n",
    "\n",
    "    '''MAIN'''\n",
    "    def main(self):\n",
    "        # make the initial graph\n",
    "        self.save_update_to_memory(self.initial_node_connector_basic())\n",
    "\n",
    "\n",
    "        for t in range(self.T):\n",
    "            self.update_simulation(self.graph_history[-1])\n",
    "\n",
    "        # calculate exposure and friending bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''INITIALISATION'''\n",
    "    # basic version: all nodes get a certain number of connections\n",
    "    def initial_node_connector_basic(self):\n",
    "        initial_graph = nx.Graph()\n",
    "\n",
    "        # add nodes until we have enough to start with\n",
    "        while len(initial_graph.nodes) < self.initial_n:\n",
    "            new_node = self.new_node_birth(initial_graph)\n",
    "            initial_graph.add_nodes_from([new_node])\n",
    "\n",
    "\n",
    "        #initial_graph.add_nodes_from(self.initial_nodes)\n",
    "\n",
    "        initial_nodes = list(initial_graph.nodes)\n",
    "\n",
    "\n",
    "        for node in initial_nodes:\n",
    "            possible_links = initial_nodes.copy()\n",
    "            possible_links.remove(node) # remove own node\n",
    "            \n",
    "            # randomly chooses m+n nodes without replacement from the initial nodes to connect (correction for taking the number of nodes instead if m+n>initial_n)\n",
    "            initial_neighbours = list(np.random.choice(possible_links, min(self.m + self.n, self.initial_n-1), replace=False))\n",
    "            \n",
    "            # creates the edges\n",
    "            initial_edges = self.helper_functions().edge_creator_helper(node, initial_neighbours)\n",
    "            initial_graph.add_edges_from(initial_edges)\n",
    "\n",
    "        return initial_graph\n",
    "\n",
    "\n",
    "    '''UPDATING'''\n",
    "    # big simulation update\n",
    "    def update_simulation(self, current_graph):\n",
    "\n",
    "\n",
    "        # copy graph with the added connections\n",
    "        new_graph = current_graph.copy()\n",
    "\n",
    "        '''maybe make this whole thing into a function'''\n",
    "        # add the new node\n",
    "        new_node = self.new_node_birth(current_graph) # DONE\n",
    "        new_graph.add_nodes_from([new_node])\n",
    "\n",
    "        # get the connections for the new node\n",
    "        parent_targets = self.find_parent_nodes_targets(self.graph_history[-1]) # DONE\n",
    "        parent_neighbour_targets = self.find_parent_neighbour_targets(self.graph_history[-1], parent_targets) # DONE\n",
    "\n",
    "\n",
    "        # add the exposure group of the node to its attributes\n",
    "        new_node_targets = list(np.concatenate((parent_targets, parent_neighbour_targets)))\n",
    "        new_node[1]['Exposure Group'].extend(new_node_targets)\n",
    "\n",
    "        # get the connections based on the targets\n",
    "        new_node_edges = self.new_node_connections(new_node, parent_targets, parent_neighbour_targets, new_graph)\n",
    "\n",
    "\n",
    "        # add the connections\n",
    "        new_graph.add_edges_from(new_node_edges)\n",
    "\n",
    "        # save and update\n",
    "        self.save_update_to_memory(new_graph)\n",
    "        self.period += 1\n",
    "\n",
    "\n",
    "    # save to history\n",
    "    def save_update_to_memory(self, updated_graph):\n",
    "        # save new graph to history\n",
    "        '''may need to make a history for t as well, if I end up not creating new nodes every iteration'''\n",
    "        self.graph_history.append(updated_graph)\n",
    "\n",
    "\n",
    "    # create new node, indexed as the next number in the list\n",
    "    def new_node_birth(self, graph, MTO_sim = False):\n",
    "        current_nodes = graph.nodes()\n",
    "        new_node = max(current_nodes) + 1 if len(current_nodes) > 0 else 1\n",
    "\n",
    "        '''currently SES determined at birth, this can be changed'''\n",
    "        new_node_SES = np.random.choice([1, 0], 1, p=[self.p_SES_high, 1-self.p_SES_high])[0]\n",
    "\n",
    "        if MTO_sim == True:\n",
    "            new_node = 'MTO'\n",
    "            new_node_SES = 0\n",
    "\n",
    "        return (new_node, {'SES_High': new_node_SES, 'Exposure Group': []})\n",
    "\n",
    "\n",
    "    # find parent nodes connections\n",
    "    def find_parent_nodes_targets(self, graph):\n",
    "        # get the current existing nodes\n",
    "        current_nodes = graph.nodes()\n",
    "\n",
    "        # get m parent nodes\n",
    "        parent_targets = np.random.choice(current_nodes, min(self.m, len(current_nodes)), replace=False)\n",
    "\n",
    "        return parent_targets\n",
    "\n",
    "\n",
    "    # find parent neighbour connections\n",
    "    def find_parent_neighbour_targets(self, graph, parent_target_list):\n",
    "        # take the list of parents and find their (unique) neighbours\n",
    "        parent_neighbours = np.unique(self.helper_functions().find_neighbours(graph, parent_target_list))\n",
    "\n",
    "        # to avoid issues: if the parents have in total less than n neighbours, take their number of neighbours instead\n",
    "        n_possible_encounters = min(len(parent_neighbours), self.n)\n",
    "\n",
    "        # get n parent neighbour nodes\n",
    "        parent_neighbour_targets = np.random.choice(parent_neighbours, n_possible_encounters, replace=False)\n",
    "\n",
    "        return parent_neighbour_targets\n",
    "\n",
    "\n",
    "    # returns a list of connections to form based on targets and probabilties\n",
    "    def new_node_connections(self, new_node, parent_target_list, parent_neighbour_target_list, graph):\n",
    "          \n",
    "        '''\n",
    "        UGLY AF list comprehension: do not use but fun\n",
    "        [np.random.choice([0,1], size=1, p=[1-prob, prob])[0] for prob in [po if edge_type == 'cross' else px for edge_type in [SES_edge_classifier(G, 1, i) for i in targets]]]\n",
    "        '''\n",
    "\n",
    "        '''parents'''\n",
    "        # probability pm to connect to each m of them\n",
    "        # first get edge types\n",
    "        parent_edge_types = [self.helper_functions().SES_edge_classifier(graph, new_node[0], i) for i in parent_target_list]\n",
    "        # get list of probs depending on the probs\n",
    "        parent_edge_probs = [self.pm_x if edge_type == 'cross' else self.pm_o for edge_type in parent_edge_types]\n",
    "        # realised outcome per prob: vector of connections\n",
    "        parent_edges = [np.random.choice([0,1], size=1, p=[1-prob, prob])[0] for prob in parent_edge_probs]\n",
    "\n",
    "        # realised connections: cross product of the two vectors\n",
    "        parent_connections = parent_target_list * parent_edges\n",
    "        parent_connections = parent_connections[parent_connections > 0] # filter out the ones with no connection\n",
    "\n",
    "        '''parent neighbours'''\n",
    "        # probability pn to connect to each n of them\n",
    "        parent_neighbour_edge_types = [self.helper_functions().SES_edge_classifier(graph, new_node[0], i) for i in parent_neighbour_target_list]\n",
    "        parent_neighbour_edge_probs = [self.pm_x if edge_type == 'cross' else self.pm_o for edge_type in parent_neighbour_edge_types]\n",
    "        parent_neighbour_edges = [np.random.choice([0,1], size=1, p=[1-prob, prob])[0] for prob in parent_neighbour_edge_probs]\n",
    "\n",
    "        # realised connections: cross product of the two vectors\n",
    "        parent_neighbour_connections = parent_neighbour_target_list * parent_neighbour_edges\n",
    "        parent_neighbour_connections = parent_neighbour_connections[parent_neighbour_connections > 0] # filter out the ones with no connection\n",
    "\n",
    "\n",
    "        '''form the connections'''\n",
    "        # combine the two\n",
    "        new_node_connections = np.concatenate((parent_connections, parent_neighbour_connections))\n",
    "        new_node_edges = self.helper_functions().edge_creator_helper(new_node[0], new_node_connections)\n",
    "\n",
    "\n",
    "        return new_node_edges\n",
    "\n",
    "\n",
    "\n",
    "    # calculates exposure and friending bias across the graph\n",
    "    def graph_exposure_friending_bias(self, graph, return_dicts = False, return_hist_lists = False, node_target = None):       \n",
    "        # calculates exposure and friending bias for a given node. Returns a dict with the two\n",
    "        def node_exposure_friending_bias(graph, node):\n",
    "            \n",
    "            # gets the exposure group of the given node: number of H nodes encountered\n",
    "            exposure_group = self.helper_functions().get_select_node_attributes(graph, 'SES_High', graph.nodes()[node]['Exposure Group'])\n",
    "\n",
    "            n_encountered = len(list(exposure_group.values()))\n",
    "            n_H_encountered = sum(list(exposure_group.values()))\n",
    "            node_SES = graph.nodes[node]['SES_High']\n",
    "\n",
    "            # remove initial nodes\n",
    "            if n_encountered == 0:\n",
    "                exposure = 0\n",
    "                friending_bias = 0\n",
    "            \n",
    "            # calculate correctly if encounter some H nodes\n",
    "            elif n_H_encountered != 0:\n",
    "                exposure = n_H_encountered / ((n_encountered)*self.p_SES_high)\n",
    "\n",
    "                '''alternative here: n_H_encountered / n_encountered'''\n",
    "                '''HAVE A SECOND LOOK AT THIS'''\n",
    "\n",
    "                neighbours_types = self.helper_functions().find_neighbour_types(graph)\n",
    "                # take the appropriate dict from the former\n",
    "                # calculate share of friends who are H\n",
    "                if node_SES == 1:\n",
    "                    friend_H_share = neighbours_types[0][node]['H Share']\n",
    "                else:\n",
    "                    friend_H_share = neighbours_types[1][node]['H Share']\n",
    "\n",
    "                # friending bias: 1 - share of H friends / share of H encounters (=0 if no bias)\n",
    "                friending_bias = 1 - friend_H_share / n_H_encountered\n",
    "            \n",
    "            else:\n",
    "                exposure = n_H_encountered / ((n_encountered)*self.p_SES_high)\n",
    "                friending_bias = 0\n",
    "\n",
    "\n",
    "            return {'Exposure Effect': exposure, 'Friending Bias': friending_bias, 'SES': node_SES}\n",
    "\n",
    "        # if requested for a specific node, just return that directly\n",
    "        if node_target != None:\n",
    "            return node_exposure_friending_bias(graph, node_target)\n",
    "\n",
    "        # list for each\n",
    "        H_expo, H_bias = [], []\n",
    "        L_expo, L_bias = [], []\n",
    "\n",
    "        node_dict = {}\n",
    "\n",
    "\n",
    "        nodes = graph.nodes()\n",
    "        for node in nodes:\n",
    "            node_expo_fb = node_exposure_friending_bias(graph, node)\n",
    "\n",
    "            if node_expo_fb['SES'] == 1:\n",
    "                H_expo.append(node_expo_fb['Exposure Effect'])\n",
    "                H_bias.append(node_expo_fb['Friending Bias'])\n",
    "            else:\n",
    "                L_expo.append(node_expo_fb['Exposure Effect'])\n",
    "                L_bias.append(node_expo_fb['Friending Bias'])\n",
    "\n",
    "            # append to the dict if requested\n",
    "            if return_dicts == True:\n",
    "                node_dict[str(node)] = node_expo_fb\n",
    "\n",
    "\n",
    "        # take averages in each category\n",
    "        H_expo_avg, H_bias_avg = sum(H_expo)/len(H_expo), sum(H_bias)/len(H_bias)\n",
    "        L_expo_avg, L_bias_avg = sum(L_expo)/len(L_expo), sum(L_bias)/len(L_bias)\n",
    "        all_expo_avg = (sum(H_expo)+sum(L_expo)) / (len(H_expo) + len(L_expo))\n",
    "        all_bias_avg = (sum(H_bias)+sum(L_bias)) / (len(H_bias) + len(L_bias))\n",
    "\n",
    "        values_dict = {'All': (all_expo_avg, all_bias_avg),\n",
    "                       'High': (H_expo_avg, H_bias_avg),\n",
    "                       'Low': (L_expo_avg, L_bias_avg)}\n",
    "        \n",
    "        return values_dict\n",
    "        '''later, incorporate returning hists etc but who cares now'''\n",
    "\n",
    "    '''MOVING TO OPPORTUNITY SIMULATION'''\n",
    "\n",
    "    # main sim\n",
    "\n",
    "    \n",
    "\n",
    "    def MTO_sim_many(self, graph, n_sims = 1):\n",
    "        # do it once\n",
    "        def MTO_sim(graph):\n",
    "\n",
    "            MTO_graph = graph.copy()\n",
    "\n",
    "            MTO_node = self.new_node_birth(graph, MTO_sim=True)\n",
    "            MTO_graph.add_nodes_from([MTO_node])\n",
    "\n",
    "            # get the connections for the new node\n",
    "            parent_targets = self.find_parent_nodes_targets(self.graph_history[-1]) # DONE\n",
    "            parent_neighbour_targets = self.find_parent_neighbour_targets(self.graph_history[-1], parent_targets) # DONE\n",
    "\n",
    "\n",
    "            # add the exposure group of the node to its attributes\n",
    "            new_node_targets = list(np.concatenate((parent_targets, parent_neighbour_targets)))\n",
    "            MTO_node[1]['Exposure Group'].extend(new_node_targets)\n",
    "\n",
    "            # get the connections based on the targets\n",
    "            new_node_edges = self.new_node_connections(MTO_node, parent_targets, parent_neighbour_targets, MTO_graph)\n",
    "\n",
    "\n",
    "            # add the connections\n",
    "            MTO_graph.add_edges_from(new_node_edges)\n",
    "\n",
    "\n",
    "            '''Calculate stuff'''\n",
    "            MTO_H_Share = self.helper_functions().find_neighbour_types(MTO_graph)[1]['MTO']['H Share']\n",
    "            Chetty_chars = self.graph_exposure_friending_bias(MTO_graph, node_target='MTO')\n",
    "            Exposure, Friend_Bias = Chetty_chars['Exposure Effect'], Chetty_chars['Friending Bias']\n",
    "            N_exposure = len(MTO_node[1]['Exposure Group'])\n",
    "            Degree = dict(MTO_graph.degree())['MTO']\n",
    "\n",
    "            return MTO_H_Share, Exposure, Friend_Bias, N_exposure, Degree\n",
    "\n",
    "        col_names = ['H_Share', 'Exposure', 'Friend_Bias', 'N_exposure', 'Degree']\n",
    "\n",
    "        # just return a dict if only one sim\n",
    "        if n_sims == 1:\n",
    "            return dict(zip(col_names, MTO_sim(graph)))\n",
    "        \n",
    "\n",
    "        list_results = [MTO_sim(graph) for i in range(n_sims)]\n",
    "\n",
    "        df_results = pd.DataFrame(list_results, columns=col_names)\n",
    "\n",
    "\n",
    "\n",
    "        return df_results\n",
    "\n",
    "\n",
    "\n",
    "    class helper_functions:\n",
    "        '''useful functions'''\n",
    "        # return the indicated attribute for a list of nodes\n",
    "        def get_select_node_attributes(self, graph, attribute, node_list):\n",
    "            attribute_dict_all_nodes = nx.get_node_attributes(graph, attribute)\n",
    "            attribute_dict = {node: attribute_dict_all_nodes[node] for node in node_list}\n",
    "\n",
    "            return attribute_dict\n",
    "\n",
    "        # creates a list of edges based on targets \n",
    "        def edge_creator_helper(self, node, connection_targets):\n",
    "            edges = [(node, connection_target) for connection_target in connection_targets]\n",
    "\n",
    "            return edges\n",
    "\n",
    "        # returns list of neighbours for all the nodes in the given list\n",
    "        def find_neighbours(self, graph, node_list):\n",
    "            if len(node_list) != 0:\n",
    "                all_neighbours_list = list(np.concatenate(([list(graph[node]) for node in node_list])))\n",
    "            else:\n",
    "                all_neighbours_list = []\n",
    "\n",
    "            return all_neighbours_list\n",
    "\n",
    "        \n",
    "        # check connection type of two connected nodes: cross(-class) or within(-class)\n",
    "        def SES_edge_classifier(self, graph, node1, node2):\n",
    "            SES_values = list(self.get_select_node_attributes(graph, 'SES_High', [node1, node2]).values())\n",
    "            link_type = 'within' if SES_values[0] == SES_values[1] else 'cross'\n",
    "\n",
    "            return link_type\n",
    "\n",
    "        # same, but now for all edges\n",
    "        def SES_edge_classifier_all(self, graph, return_counts = False):\n",
    "            edges = graph.edges()\n",
    "\n",
    "            list_edge_type = [self.SES_edge_classifier(graph, edge[0], edge[1]) for edge in edges]\n",
    "\n",
    "            dict_edge_type = dict(zip(edges, list_edge_type))\n",
    "            dict_edge_type = {edge: {'Edge type': dict_edge_type[edge]} for edge in edges}\n",
    "\n",
    "\n",
    "            # counts the number of each\n",
    "            if return_counts == True:\n",
    "                edge_type_counts = np.asarray(np.unique(self.SES_edge_classifier_all(graph)[0], return_counts=True)).T\n",
    "                \n",
    "\n",
    "            if return_counts == True:\n",
    "                return list_edge_type, dict_edge_type, edge_type_counts\n",
    "            \n",
    "            else:\n",
    "                return list_edge_type, dict_edge_type\n",
    "\n",
    "\n",
    "        # returns a tuple: \n",
    "        # 0: Pearson correlation coefficient between a node's degree and the average clustering of its neighbours\n",
    "        # 1: Dict with each node's degree and the average correlation of its neighbours\n",
    "        # 2: List with two lists: degree and neighbour clustering for each node (ex for plotting)\n",
    "        def degree_neighbour_clustering_corr(self, graph):\n",
    "            nodes = list(graph.nodes())\n",
    "            node_dict = {}\n",
    "            \n",
    "            # calculate degree and clustering at the beginning\n",
    "            node_degree = dict(nx.degree(graph))\n",
    "            node_clustering = dict(nx.clustering(graph))\n",
    "\n",
    "            # iterate over nodes\n",
    "            for node in nodes:\n",
    "                # find the neighbours\n",
    "                node_neighbours = self.find_neighbours(graph, [node])\n",
    "                neighbour_clustering = []\n",
    "\n",
    "                # get clustering for each neighbour and calculate the average\n",
    "                for neighbour in node_neighbours:\n",
    "                    neighbour_clustering.append(node_clustering[neighbour])\n",
    "                avg_neighbour_clustering = np.average(neighbour_clustering)\n",
    "\n",
    "                # append to the dict: tuple with node's own degree, and its neighbours avg clustering\n",
    "                node_dict[node] = (node_degree[node], avg_neighbour_clustering)\n",
    "\n",
    "            # calculate correlation across the nodes\n",
    "            data = np.transpose(list(node_dict.values()))\n",
    "            corr = np.corrcoef(data)[0][1]\n",
    "\n",
    "\n",
    "            return (corr, node_dict, data)\n",
    "\n",
    "\n",
    "        # returns a dict with the characteristics of social networks\n",
    "        def graph_social_network_chars(self, graph, summarise = True):\n",
    "            # summarising function\n",
    "            def summ_graph_social_network_chars(GSNC_output):\n",
    "                summary_dict = {\n",
    "                    'Diameter': GSNC_output['Diameter'],\n",
    "                    'APL': GSNC_output['APL'],\n",
    "                    'Clustering': GSNC_output['Clustering'],\n",
    "                    'Degree': GSNC_output['Degree'],\n",
    "                    'Assortativity': GSNC_output['Assortativity'],\n",
    "                    'Degree-neighbour-clustering': GSNC_output['Degree-neighbour-clustering'][0]\n",
    "                }\n",
    "\n",
    "                return summary_dict\n",
    "            \n",
    "            degree_hist = nx.degree_histogram(graph)\n",
    "            avg_degree = np.average(degree_hist)\n",
    "            var_degree = np.var(degree_hist)\n",
    "\n",
    "            small_world_char_dict = {\n",
    "                'Diameter': nx.diameter(graph),     # low diameter\n",
    "                'APL': nx.average_shortest_path_length(graph),      # relatively low APL (order of log n)\n",
    "                'Clustering': nx.average_clustering(graph),     # relatively high clustering\n",
    "                'Degree': (avg_degree, var_degree),     # fat degree tails\n",
    "                'Degree histogram': degree_hist,\n",
    "                #nx.degree_histogram(G) # returns a list a where a[x] = number of nodes with a degree of x, up until the highest amount     \n",
    "                'Assortativity': nx.assortativity.degree_assortativity_coefficient(graph),      # assortativity: positive degree correlation\n",
    "                'Degree-neighbour-clustering': self.degree_neighbour_clustering_corr(graph)     # negative correlation between own degree and neighbours' clustering\n",
    "            }\n",
    "\n",
    "            # can return more detailed\n",
    "            if summarise == True:\n",
    "                return summ_graph_social_network_chars(small_world_char_dict)\n",
    "            else:\n",
    "                return small_world_char_dict\n",
    "\n",
    "\n",
    "        # finds the number of H and L friends for each node, as well as the share\n",
    "        # returns 2 dicts: one for H, one for L; each containing [0] H share, [1] H neighbours, [2] L neighbours\n",
    "        def find_neighbour_types(self, graph):\n",
    "\n",
    "            # sort all nodes into either H or L\n",
    "            H_nodes = []\n",
    "            L_nodes = []\n",
    "\n",
    "            node_degrees = graph.degree()\n",
    "\n",
    "            for node, attrs in graph.nodes(data=True):\n",
    "                if attrs['SES_High'] == 1:\n",
    "                    H_nodes.append(node)\n",
    "                else:\n",
    "                    L_nodes.append(node)\n",
    "\n",
    "            H_graph_dict = {}\n",
    "            L_graph_dict = {}\n",
    "\n",
    "            # loop over nodes\n",
    "            for node in graph.nodes():\n",
    "                neighbours = graph.neighbors(node)\n",
    "                H_neighbours = []\n",
    "                L_neighbours = []\n",
    "\n",
    "                # loop over each node's neighbours\n",
    "                # sort into one of two lists\n",
    "                for neighbour in neighbours:\n",
    "                    if neighbour in H_nodes:\n",
    "                        H_neighbours.append(neighbour)\n",
    "                    else:\n",
    "                        L_neighbours.append(neighbour)\n",
    "\n",
    "                H_share = len(H_neighbours) / node_degrees[node] if node_degrees[node] > 0 else 0\n",
    "\n",
    "                # put into a dict for each node: share of H friends, number of H and low friends\n",
    "                node_dict = {'H Share': H_share, 'NH': len(H_neighbours), 'NL': len(L_neighbours)}\n",
    "\n",
    "                # append this dict for a dict of the whole graph\n",
    "                if node in H_nodes:\n",
    "                    H_graph_dict[node] = node_dict\n",
    "                else:\n",
    "                    L_graph_dict[node] = node_dict\n",
    "\n",
    "\n",
    "            return (H_graph_dict, L_graph_dict)\n",
    "\n",
    "        # returns a tuple: average share of H friends for H and L\n",
    "        def average_neighbour_type_per_SES(self, graph, return_dicts = False, return_hist_lists = False):\n",
    "            H_dict, L_dict = self.find_neighbour_types(graph)\n",
    "\n",
    "            H_share_list = [H_dict[node]['H Share'] for node in H_dict]\n",
    "            L_share_list = [L_dict[node]['H Share'] for node in L_dict]\n",
    "\n",
    "\n",
    "            # if H and L nodes both exist, calculate averages, otherwise just 1\n",
    "            if len(list(H_dict.keys())) != 0 and len(list(L_dict.keys())) != 0:\n",
    "                H_average = sum(H_share_list) / len(list(H_dict.keys()))\n",
    "                L_average = sum(L_share_list) / len(list(L_dict.keys()))\n",
    "\n",
    "            elif len(list(H_dict.keys())) == 0:\n",
    "                H_average = 0\n",
    "                L_average = 1\n",
    "            \n",
    "            else:\n",
    "                H_average = 1\n",
    "                L_average = 0\n",
    "\n",
    "\n",
    "            if return_dicts == True:\n",
    "                if return_hist_lists == True:\n",
    "                    return ((H_average, L_average), (H_dict, L_dict), (H_share_list, L_share_list))\n",
    "                else:\n",
    "                    return ((H_average, L_average), (H_dict, L_dict))\n",
    "                \n",
    "            else:\n",
    "                if return_hist_lists == True:\n",
    "                    return ((H_average, L_average), (H_share_list, L_share_list))\n",
    "                else:\n",
    "                    return (H_average, L_average)\n",
    "            # return dicts as well for the plotly drawing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class plotly_sim_drawing(object):\n",
    "    '''HELPER FUNCTIONS'''\n",
    "    '''MODIFY THESE TO TAKE A SPECIFIC TIME INSTEAD'''\n",
    "    # gets a tuple of arrays of edge positions\n",
    "    def go_get_edge_positions(self, graph, graph_layout):\n",
    "\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "\n",
    "        for edge in graph.edges():\n",
    "            x0, y0 = graph_layout[edge[0]]\n",
    "            x1, y1 = graph_layout[edge[1]]\n",
    "            edge_x.append(x0)\n",
    "            edge_x.append(x1)\n",
    "            edge_x.append(None)\n",
    "            edge_y.append(y0)\n",
    "            edge_y.append(y1)\n",
    "            edge_y.append(None)\n",
    "\n",
    "\n",
    "        return (edge_x, edge_y)\n",
    "\n",
    "    # gets a edge trace based on a wanted layout and graph\n",
    "    #to add color list later: https://stackoverflow.com/questions/62601052/option-to-add-edge-colouring-in-networkx-trace-using-plotly\n",
    "\n",
    "    def go_get_edge_trace(self, graph, graph_layout):#, edge_color_list):\n",
    "        edge_x, edge_y = self.go_get_edge_positions(graph, graph_layout)\n",
    "\n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            hoverinfo=None,\n",
    "            mode='lines',\n",
    "\n",
    "            line=dict(\n",
    "                width=0.5, \n",
    "                color='#888')\n",
    "        )\n",
    "\n",
    "        return edge_trace\n",
    "\n",
    "\n",
    "    # returns a tuple with an array for each of x pos and y pos\n",
    "    def go_get_node_positions(self, graph, graph_layout):\n",
    "\n",
    "        node_x_list = []\n",
    "        node_y_list = []\n",
    "\n",
    "        for node in graph.nodes():\n",
    "            node_x, node_y = graph_layout[node]\n",
    "            node_x_list.append(node_x)\n",
    "            node_y_list.append(node_y)\n",
    "\n",
    "\n",
    "        return (node_x_list, node_y_list)\n",
    "\n",
    "    # gets a node trace based on a wanted layout and graph\n",
    "    def go_get_node_trace(self, graph, graph_layout, node_color_list, node_text):\n",
    "        node_x, node_y = self.go_get_node_positions(graph, graph_layout)\n",
    "\n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            hoverinfo='text',\n",
    "            mode='markers',\n",
    "            text=node_text,\n",
    "            #IDEA: add text for number of friends in each group\n",
    "\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=node_color_list),\n",
    "        )\n",
    "\n",
    "        return node_trace\n",
    "\n",
    "\n",
    "    def get_legend_text(self, simulation, graph):\n",
    "        # share of H friends\n",
    "        H_shares = simulation.helper_functions().average_neighbour_type_per_SES(graph)\n",
    "        text_H_shares = f\"Mean share of High-SES friends:<br> • High SES: {round(H_shares[0], 2)}<br> • Low SES: {round(H_shares[1], 2)}<br>\"\n",
    "\n",
    "        # attribute assortativity\n",
    "        attr_assortativity = nx.assortativity.attribute_assortativity_coefficient(graph, 'SES_High')\n",
    "        text_attr_assortativity = f\"High SES assortativity: {round(attr_assortativity, 2)}\"\n",
    "\n",
    "        # edge types\n",
    "        edge_type_counts = simulation.helper_functions().SES_edge_classifier_all(graph, return_counts = True)[2]\n",
    "        \n",
    "        within_count, cross_count = int(edge_type_counts[0][1]), int(edge_type_counts[1][1])\n",
    "        total_count = within_count + cross_count\n",
    "        text_edge_type_counts = f\"<br>Edge types<br> • Within-SES: {round(within_count / total_count, 2)}<br> • Across-SES: {round(cross_count / total_count, 2)}<br> • Total: {total_count}\"\n",
    "\n",
    "\n",
    "        # largest connected component / unconnected nodes\n",
    "        n_in_largest_cc = len(max(nx.connected_components(graph), key=len))\n",
    "        text_largest_cc = f\"<br> Nodes in largest CC: {n_in_largest_cc} / {simulation.T + simulation.initial_n}\"\n",
    "\n",
    "        # expected number of connections per node without bias vs realised average degree and variance\n",
    "        degree_hist = nx.degree_histogram(graph)\n",
    "        avg_degree = round(np.average(degree_hist), 2)\n",
    "        var_degree = round(np.var(degree_hist), 2)\n",
    "\n",
    "        no_bias_expected_degree = simulation.m*simulation.pm_o + simulation.n*simulation.pn_o\n",
    "        realised_degree_dist = (avg_degree, var_degree)\n",
    "        text_encpn = f\"<br>Expected average degree <br>without bias: {no_bias_expected_degree}<br>Realised degree <br>mean, var: {realised_degree_dist}\"\n",
    "\n",
    "\n",
    "        # parameters?\n",
    "        text_parameters = f\"<br><br><br>Parameters<br> • Initial nodes: {simulation.initial_n}<br> • T: {simulation.T}<br> • m: {simulation.m}<br> • pm_o: {simulation.pm_o}<br> • n: {simulation.n}<br> • pn_o: {simulation.pn_o}<br> • p_SES_high: {simulation.p_SES_high}<br> • rho: {simulation.rho}<br> • pm_x: {simulation.pm_o / simulation.rho}<br> • pn_x: {simulation.pn_o / simulation.rho}\"\n",
    "\n",
    "        # combine everything\n",
    "        all_texts = [text_H_shares, text_attr_assortativity, text_edge_type_counts, text_encpn, text_largest_cc, text_parameters]\n",
    "        \n",
    "        final_text = '<br>'.join(all_texts)\n",
    "        return final_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''ACTUAL DRAWINGS'''\n",
    "\n",
    "    # basic plotly draw\n",
    "    def plotly_draw(self, simulation, t=-1, layout='spring', draw_largest_CC=True, legend=False, title = None):\n",
    "        graph = simulation.graph_history[t]\n",
    "\n",
    "        # only draw the largest connected component (since there are always a few nodes with no connections which makes the graph ugly)\n",
    "        if draw_largest_CC == True:\n",
    "            largest_CC = max(nx.connected_components(graph), key=len)\n",
    "            graph = graph.subgraph(largest_CC).copy()\n",
    "\n",
    "        SES_list = list(simulation.helper_functions().get_select_node_attributes(graph, 'SES_High', graph.nodes()).values())\n",
    "        SES_color_list = ['blue' if SES == 1 else 'red' for SES in SES_list]\n",
    "\n",
    "\n",
    "        # node texts\n",
    "        '''also add share of H/L neighbour and degree'''\n",
    "        node_text = [str(i) + '_' + str(j) for i, j in zip(list(graph.nodes()), SES_list)]\n",
    "\n",
    "        graph_layout = nx.spring_layout(graph, seed=42, scale=10) if layout =='spring' else nx.random_layout(graph)\n",
    "\n",
    "        edge_trace = self.go_get_edge_trace(graph, graph_layout)\n",
    "        node_trace = self.go_get_node_trace(graph, graph_layout, SES_color_list, node_text)\n",
    "\n",
    "\n",
    "        # Get annotations if needed\n",
    "        if legend == True:\n",
    "            # legend: parameters, realised values of a bunch of things\n",
    "            legend_text = self.get_legend_text(simulation, graph)\n",
    "            annotations = [dict(\n",
    "                            text=legend_text,\n",
    "                            align='left',\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=1.35, y=1)]\n",
    "            l_margin = 205\n",
    "            width, height = 800, 600\n",
    "        \n",
    "        else:\n",
    "            annotations = []\n",
    "            l_margin = 5\n",
    "            width, height = 600, 600\n",
    "\n",
    "\n",
    "        # make custom title\n",
    "        if title == None:\n",
    "            title_dict = None\n",
    "            t_margin = 5\n",
    "\n",
    "        else:\n",
    "            title = f'' if title == '' else title\n",
    "            title_dict = {'text': title, 'x': 0.5, 'y': 0.98}\n",
    "            t_margin = 30\n",
    "\n",
    "        '''actual figure'''\n",
    "        fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=title_dict,\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=5,l=5,r=l_margin,t=t_margin),\n",
    "                        annotations=annotations,\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        width=width, height=height)                      \n",
    "                        )\n",
    "\n",
    "\n",
    "        return fig\n",
    "\n",
    "\n",
    "#plotly_sim_drawing.plotly_draw(plotly_sim_drawing(), new_sim, -1, layout='spring', draw_largest_CC=True, title=None, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Programming\\Code\\Dissertation\\Diss model\\Diss_model.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vals \u001b[39m=\u001b[39m new_sim\u001b[39m.\u001b[39;49mhelper_functions()\u001b[39m.\u001b[39;49maverage_neighbour_type_per_SES(new_sim\u001b[39m.\u001b[39;49mgraph_history[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], return_hist_lists \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bins \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#plt.hist(H_vals, bins, alpha=0.5, label='High SES')\u001b[39;00m\n",
      "\u001b[1;32mc:\\Programming\\Code\\Dissertation\\Diss model\\Diss_model.ipynb Cell 9\u001b[0m in \u001b[0;36mJacksonSimulationV2.helper_functions.average_neighbour_type_per_SES\u001b[1;34m(self, graph, return_dicts, return_hist_lists)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=377'>378</a>\u001b[0m L_share_list \u001b[39m=\u001b[39m [L_dict[node][\u001b[39m'\u001b[39m\u001b[39mH Share\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m L_dict]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=380'>381</a>\u001b[0m H_average \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39msum\u001b[39m(H_share_list) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(H_dict\u001b[39m.\u001b[39mkeys())), \u001b[39m0\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=381'>382</a>\u001b[0m L_average \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39msum\u001b[39;49m(L_share_list) \u001b[39m/\u001b[39;49m \u001b[39mlen\u001b[39;49m(\u001b[39mlist\u001b[39;49m(L_dict\u001b[39m.\u001b[39;49mkeys())), \u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=383'>384</a>\u001b[0m \u001b[39mif\u001b[39;00m return_dicts \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Programming/Code/Dissertation/Diss%20model/Diss_model.ipynb#X11sZmlsZQ%3D%3D?line=384'>385</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_hist_lists \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "vals = new_sim.helper_functions().average_neighbour_type_per_SES(new_sim.graph_history[-1], return_hist_lists = True)[1]\n",
    "\n",
    "bins = np.linspace(0, 1, 8)\n",
    "#plt.hist(H_vals, bins, alpha=0.5, label='High SES')\n",
    "plt.hist(vals, bins, alpha=0.5, label='Low SES')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'H_Share'}>,\n",
       "        <AxesSubplot:title={'center':'Exposure'}>],\n",
       "       [<AxesSubplot:title={'center':'Friend_Bias'}>,\n",
       "        <AxesSubplot:title={'center':'N_exposure'}>],\n",
       "       [<AxesSubplot:title={'center':'Degree'}>, <AxesSubplot:>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# parameter selection\n",
    "\n",
    "# initial nodes and time\n",
    "initial_n = 0\n",
    "T = 250\n",
    "\n",
    "p = 0.25\n",
    "\n",
    "# parent nodes and prob\n",
    "m = 8\n",
    "pm_o = p\n",
    "\n",
    "# parent neighbours and prob\n",
    "n = 4\n",
    "pn_o = p\n",
    "\n",
    "rho = 3\n",
    "\n",
    "# p high SES\n",
    "p_SES_high = 0.5\n",
    "\n",
    "new_sim = JacksonSimulationV2(initial_n, T, m, pm_o, n, pn_o, p_SES_high, biased=True, rho=rho)\n",
    "\n",
    "\n",
    "#plotly_sim_drawing.plotly_draw(plotly_sim_drawing(), new_sim, layout='spring', draw_largest_CC=True, title=f'Network after {T} iterations, rho={rho}')\n",
    "\n",
    "\n",
    "stats = new_sim.MTO_sim_many(new_sim.graph_history[-1], n_sims=1000)\n",
    "stats.head(5)\n",
    "stats.hist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "289e6dbf0cbe13abb58a8c6441fb07dbab3e022f897dda6d31a08b19d28acfde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
